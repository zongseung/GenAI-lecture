"""
ì „ë ¥ìˆ˜ìš” ë°ì´í„° ë¶„ì„ ë©€í‹°ì—ì´ì „íŠ¸ ì±—ë´‡ ì‹œìŠ¤í…œ - Streamlit UI
"""
import streamlit as st
import os
import locale
import unicodedata
import pandas as pd
import sqlite3
import plotly.graph_objects as go
import plotly.express as px
import urllib.request as _urlreq
import urllib.error as _urlerr

# ===== UTF-8 í™˜ê²½ ê°•ì œ (ì¶œë ¥ ì¸ì½”ë”© ì˜¤ë¥˜ ë°©ì§€) =====
os.environ.setdefault("PYTHONIOENCODING", "utf-8")
os.environ.setdefault("LC_ALL", "C.UTF-8")
try:
    locale.setlocale(locale.LC_ALL, "C.UTF-8")
except Exception:
    pass

# ===== í˜ì´ì§€ ì„¤ì • =====
st.set_page_config(
    page_title="âš¡ ì „ë ¥ìˆ˜ìš” AI ë¶„ì„ ì‹œìŠ¤í…œ",
    page_icon="âš¡",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ===== ìŠ¤íƒ€ì¼ =====
st.markdown("""
<style>
    .main-header {
        font-size: 3rem;
        font-weight: bold;
        text-align: center;
        background: linear-gradient(90deg, #667eea 0%, #764ba2 100%);
        -webkit-background-clip: text;
        -webkit-text-fill-color: transparent;
        margin-bottom: 2rem;
    }
    .metric-card { background: white; padding: 1rem; border-radius: 10px; border-left: 4px solid #667eea; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
</style>
""", unsafe_allow_html=True)

st.markdown('<h1 class="main-header">âš¡ ì „ë ¥ìˆ˜ìš” AI ë¶„ì„ ì±—ë´‡</h1>', unsafe_allow_html=True)

# ===== ì„¤ì •/ê°€ì ¸ì˜¤ê¸° =====
import sys
sys.path.append('src')

from config import OPENAI_API_KEY, DATABASE_PATH
from src.agents.supervisor import SupervisorAgent
from src.workflow import EnergyLLMWorkflow
# LangGraph ì‹œê°í™” ê¸°ëŠ¥ ì œê±° (ë Œë”ë§ ì˜¤ë¥˜ íšŒí”¼)
from src.utils.graph_visualizer import create_workflow_images
from src.utils.mermaid_visualizer import (
    render_langgraph_workflow,
    create_mermaid_workflow_images,
    create_langgraph_native_workflow,
)

def safe_str(text):
    """UI í‘œì‹œ ì•ˆì •í™”ë¥¼ ìœ„í•œ ì •ê·œí™”(ë°ì´í„° ê°€ê³µ ê¸ˆì§€)."""
    if not isinstance(text, str):
        try: 
            text = str(text)
        except Exception: 
            return ""
    
    # ê°•ì œ UTF-8 ì¸ì½”ë”©/ë””ì½”ë”©ìœ¼ë¡œ ASCII ì˜¤ë¥˜ ë°©ì§€
    try:
        text = text.encode('utf-8', errors='replace').decode('utf-8')
    except Exception:
        text = str(text).encode('ascii', errors='replace').decode('ascii')
    
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
    try:
        text = unicodedata.normalize('NFKC', text)
    except Exception:
        pass
    
    # íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì¹˜í™˜
    repl = {
        '\u201c': '"', '\u201d': '"', '\u2018': "'", '\u2019': "'", 
        '\u2013': '-', '\u2014': '-', '\u2026': '...', '\u00a0': ' ',
        '\u2019': "'", '\u2018': "'", '\u201c': '"', '\u201d': '"'
    }
    for k, v in repl.items():
        text = text.replace(k, v)
    
    return text

# ===== GPU/CUDA ê°ì§€ ìœ í‹¸ =====
def detect_cuda_status():
    info = {"available": False, "via": [], "devices": []}
    # 1) PyTorch
    try:
        import torch as _torch
        if _torch.cuda.is_available():
            info["available"] = True
            info["via"].append("torch")
            try:
                cnt = _torch.cuda.device_count()
                info["devices"] = [
                    _torch.cuda.get_device_name(i) for i in range(cnt)
                ]
            except Exception:
                pass
    except Exception:
        pass
    # 2) nvidia-smi
    try:
        import subprocess as _sp
        out = _sp.check_output(
            ["nvidia-smi", "--query-gpu=name,memory.total", "--format=csv,noheader"],
            stderr=_sp.DEVNULL,
            timeout=1.5,
            text=True,
        ).strip()
        if out:
            info["available"] = True
            info["via"].append("nvidia-smi")
            lines = [l.strip() for l in out.splitlines() if l.strip()]
            info["devices"].extend(lines)
    except Exception:
        pass
    # 3) /dev ê²€ì‚¬
    try:
        import os as _os
        devs = [p for p in _os.listdir("/dev") if p.startswith("nvidia")]  # nvidia0, nvidiactl ë“±
        if devs:
            info["available"] = True
            info["via"].append("/dev/nvidia*")
    except Exception:
        pass
    return info

if not OPENAI_API_KEY:
    st.error("âš ï¸ í™˜ê²½ë³€ìˆ˜ ë˜ëŠ” .env ì— OPENAI_API_KEYë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
    st.stop()

# ===== LangGraph ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” =====
@st.cache_resource
def initialize_workflow():
    """ê¸°ë³¸ LangGraph ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™”"""
    try:
        return EnergyLLMWorkflow(
            db_path=DATABASE_PATH,
            openai_api_key=OPENAI_API_KEY
        )
    except Exception as e:
        st.error(f"LangGraph ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return None

@st.cache_resource
def get_workflow_with_backend(backend: str, ollama_url: str | None, 
                             sql_model: str, ml_model: str,
                             ollama_sql_model: str | None = None, 
                             ollama_ml_model: str | None = None):
    """ë°±ì—”ë“œë³„ LangGraph ì›Œí¬í”Œë¡œìš° ìƒì„±"""
    try:
        return EnergyLLMWorkflow(
            db_path=DATABASE_PATH,
            openai_api_key=OPENAI_API_KEY,
            backend=backend,
            ollama_base_url=ollama_url,
            sql_model=sql_model,
            ml_model=ml_model,
            ollama_sql_model=ollama_sql_model,
            ollama_ml_model=ollama_ml_model,
        )
    except Exception as e:
        st.error(f"ë°±ì—”ë“œë³„ LangGraph ì›Œí¬í”Œë¡œìš° ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return None

default_workflow = initialize_workflow()
if not default_workflow:
    st.stop()

# ===== ë ˆê±°ì‹œ Supervisor ì´ˆê¸°í™” =====
@st.cache_resource
def initialize_supervisor():
    try:
        return SupervisorAgent(db_path=DATABASE_PATH, openai_api_key=OPENAI_API_KEY)
    except Exception as e:
        st.error(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
        return None

supervisor = initialize_supervisor()
if not supervisor:
    st.stop()

# ë°±ì—”ë“œ ì˜µì…˜ë³„ Supervisor ì´ˆê¸°í™” (ìºì‹œ)
@st.cache_resource
def get_supervisor_with_backend(backend: str, ollama_url: str | None, 
                               sql_model: str, ml_model: str,
                               ollama_sql_model: str | None = None, 
                               ollama_ml_model: str | None = None):
    try:
        return SupervisorAgent(
            db_path=DATABASE_PATH,
            openai_api_key=OPENAI_API_KEY,
            backend=backend,
            ollama_base_url=ollama_url,
            sql_model=sql_model,
            ml_model=ml_model,
            ollama_sql_model=ollama_sql_model,
            ollama_ml_model=ollama_ml_model,
        )
    except Exception as e:
        st.error(f"ì—ì´ì „íŠ¸ ì´ˆê¸°í™” ì˜¤ë¥˜(ë°±ì—”ë“œ): {e}")
        return None

# ë°ì´í„° ë¡œë”©
@st.cache_data
def load_head_data(db_path: str, limit: int = 1000) -> pd.DataFrame:
    try:
        conn = sqlite3.connect(db_path)
        df = pd.read_sql_query(f"SELECT * FROM my_table ORDER BY \"time\" DESC LIMIT {limit}", conn)
        conn.close()
        if 'time' in df.columns:
            df['time'] = pd.to_datetime(df['time'])
        return df
    except Exception as e:
        st.error(f"ë°ì´í„° ë¡œë”© ì˜¤ë¥˜: {e}")
        return pd.DataFrame()

# ì „ì²´ ë°ì´í„° ìˆ˜ í™•ì¸ì„ ìœ„í•œ í•¨ìˆ˜
@st.cache_data
def get_total_count(db_path: str) -> int:
    try:
        conn = sqlite3.connect(db_path)
        result = conn.execute("SELECT COUNT(*) FROM my_table").fetchone()
        conn.close()
        return result[0] if result else 0
    except Exception:
        return 0

data = load_head_data(DATABASE_PATH, 1000)
total_count = get_total_count(DATABASE_PATH)

# ===== ì‚¬ì´ë“œë°” =====
with st.sidebar:
    st.markdown("**ğŸ”‘ API ì„¤ì •**")
    st.success("âœ… API í‚¤ ì„¤ì •ë¨" if OPENAI_API_KEY else "âŒ API í‚¤ ì—†ìŒ")

    st.markdown("---")
    st.markdown("**ğŸ¤– ì‚¬ìš© ê°€ëŠ¥í•œ ì—ì´ì „íŠ¸**")
    st.markdown("â€¢ **SQL ë¶„ì„ê°€**: ë°ì´í„° íƒìƒ‰/ë¶„ì„")
    st.markdown("â€¢ **ML ì—”ì§€ë‹ˆì–´**: ëª¨ë¸ ì½”ë“œ ìƒì„±")
    st.markdown("â€¢ **ìˆ˜í¼ë°”ì´ì €**: ë¼ìš°íŒ…/í†µí•©")

    st.markdown("---")
    st.markdown("**LLM ë°±ì—”ë“œ**")
    backend = st.selectbox("ë°±ì—”ë“œ ì„ íƒ", ["OpenAI", "Ollama"], index=0)
    
    # ì—ì´ì „íŠ¸ë³„ ëª¨ë¸ ì„¤ì •
    st.markdown("**ì—ì´ì „íŠ¸ë³„ ëª¨ë¸ ì„¤ì •**")
    
    if backend == "OpenAI":
        sql_model = st.selectbox("SQL ë¶„ì„ê°€ ëª¨ë¸", 
                                ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"], 
                                index=0, key="sql_openai_model")
        ml_model = st.selectbox("ML ì—”ì§€ë‹ˆì–´ ëª¨ë¸", 
                               ["gpt-4o-mini", "gpt-4o", "gpt-3.5-turbo"], 
                               index=0, key="ml_openai_model")
        ollama_url = None
        ollama_sql_model = None
        ollama_ml_model = None
    else:  # Ollama
        ollama_url = st.text_input("Ollama Base URL", value="http://localhost:11434")
        sql_model = st.text_input("SQL ë¶„ì„ê°€ Ollama ëª¨ë¸", value="llama3.1:8b", key="sql_ollama_model")
        ml_model = st.text_input("ML ì—”ì§€ë‹ˆì–´ Ollama ëª¨ë¸", value="qwen2.5:72b", key="ml_ollama_model")
        ollama_sql_model = sql_model
        ollama_ml_model = ml_model

    # Ollama ì—°ê²° ì‚¬ì „ ì ê²€ ë° í´ë°±
    effective_backend = "openai"
    if backend == "Ollama" and ollama_url:
        try:
            _ = _urlreq.urlopen(f"{ollama_url.rstrip('/')}/api/tags", timeout=1.5)
            st.success("Ollama ì—°ê²° ì„±ê³µ")
            effective_backend = "ollama"
        except Exception:
            st.warning("Ollamaì— ì—°ê²°í•  ìˆ˜ ì—†ì–´ OpenAIë¡œ í´ë°±í•©ë‹ˆë‹¤. (Base URL í™•ì¸)")
            effective_backend = "openai"

    # CUDA ê°ì§€ (ë‹¤ì¤‘ ì „ëµ)
    cuda = detect_cuda_status()
    if cuda.get("available"):
        devices = cuda.get("devices") or []
        via = ", ".join(cuda.get("via", [])) or "unknown"
        label = " | ".join(devices) if devices else "GPU Detected"
        st.success(f"CUDA ê°ì§€ë¨ ({via}) - {label}")
    else:
        st.info("CUDA ë¯¸ê°ì§€ (CPU ëª¨ë“œ)")

    st.markdown("---")
    st.markdown("**ğŸ”„ LangGraph ì›Œí¬í”Œë¡œìš°**")
    
    workflow_option = st.radio(
        "ì‹œê°í™” ë°©ì‹ ì„ íƒ:",
        ["ğŸ¯ LangGraph ë„¤ì´í‹°ë¸Œ", "ğŸ“Š Mermaid ë‹¤ì´ì–´ê·¸ë¨", "ğŸ–¼ï¸ Graphviz ì´ë¯¸ì§€"],
        horizontal=True
    )
    
    if st.button("ğŸ”„ ì›Œí¬í”Œë¡œìš° ìƒì„±"):
        with st.spinner("LangGraph ë‹¤ì´ì–´ê·¸ë¨ ìƒì„± ì¤‘..."):
            try:
                if workflow_option == "ğŸ¯ LangGraph ë„¤ì´í‹°ë¸Œ":
                    result = create_langgraph_native_workflow(default_workflow)
                    if result.get("success"):
                        st.success("âœ… LangGraph ë„¤ì´í‹°ë¸Œ ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
                    else:
                        st.error(f"âŒ ë„¤ì´í‹°ë¸Œ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {result.get('error')}")
                elif workflow_option == "ğŸ“Š Mermaid ë‹¤ì´ì–´ê·¸ë¨":
                    result = create_mermaid_workflow_images()
                    if result.get("success"):
                        st.success("âœ… Mermaid ì›Œí¬í”Œë¡œìš° ìƒì„± ì™„ë£Œ")
                    else:
                        st.error(f"âŒ Mermaid ì˜¤ë¥˜: {result.get('error')}")
                else:
                    results = create_workflow_images()
                    if results.get("basic_workflow"):
                        st.success("âœ… Graphviz ì´ë¯¸ì§€ ìƒì„± ì™„ë£Œ")
                        st.image(results["basic_workflow"], caption="LangGraph ë©€í‹°ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš°", use_column_width=True)
                    else:
                        st.warning("âš ï¸ graphviz ì„¤ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
            except Exception as e:
                st.error(f"âŒ ì›Œí¬í”Œë¡œìš° ìƒì„± ì˜¤ë¥˜: {e}")

    st.markdown("---")
    st.markdown("**ğŸ’¡ ì˜ˆì‹œ ì§ˆë¬¸**")
    st.markdown("**ğŸ“Š ë°ì´í„° ë¶„ì„:**")
    st.markdown("â€¢ ìµœê·¼ 1ì£¼ì¼ ì „ë ¥ìˆ˜ìš” ì¶”ì„¸ì™€ í”¼í¬ëŠ”?")
    st.markdown("â€¢ ê¸°ì˜¨(ta)ê³¼ ì „ë ¥ìˆ˜ìš”ì˜ ìƒê´€ê´€ê³„ ë¶„ì„")
    st.markdown("â€¢ ìš”ì¼/ì£¼ë§/ê³µíœ´ì¼ì— ë”°ë¥¸ í‰ê·  ìˆ˜ìš” ë¹„êµ")
    st.markdown("â€¢ ì–´ì œ ê°€ì¥ ìˆ˜ìš”ê°€ ë†’ì•˜ë˜ ì‹œê°„ì€?")
    st.markdown("**ğŸ§  ëª¨ë¸ ì½”ë“œ ìƒì„±:**")
    st.markdown("â€¢ LSTMìœ¼ë¡œ ì „ë ¥ìˆ˜ìš” ì˜ˆì¸¡ ì½”ë“œ ìƒì„±í•´ì¤˜")
    st.markdown("â€¢ scikit-learn íšŒê·€ ê¸°ë°˜ ìˆ˜ìš” ì˜ˆì¸¡ ëª¨ë¸")
    st.markdown("â€¢ XGBoost ìˆ˜ìš” ì˜ˆì¸¡ ì½”ë“œ ì‘ì„±í•´ì¤˜")

# ===== ìƒë‹¨ ë©”íŠ¸ë¦­ =====
if not data.empty:
    c1,c2,c3,c4 = st.columns(4)
    with c1: st.metric("ì´ ë°ì´í„° ìˆ˜", f"{total_count:,}", help=f"í‘œì‹œëœ ë°ì´í„°: {len(data):,}ê°œ")
    with c2:
        if 'power demand(MW)' in data.columns:
            latest_demand = data['power demand(MW)'].iloc[0]
            st.metric("ìµœì‹  ìˆ˜ìš”(MW)", f"{latest_demand:,.0f}")
        else:
            st.metric("ìµœì‹  ìˆ˜ìš”(MW)", "N/A")
    with c3:
        if 'power demand(MW)' in data.columns and len(data) > 1 and data['power demand(MW)'].iloc[1] != 0:
            demand_change = (data['power demand(MW)'].iloc[0] - data['power demand(MW)'].iloc[1]) / data['power demand(MW)'].iloc[1] * 100
            st.metric("ìˆ˜ìš” ë³€ë™ë¥ ", f"{demand_change:.2f}%", delta=f"{demand_change:.2f}%")
        else:
            st.metric("ìˆ˜ìš” ë³€ë™ë¥ ", "N/A")
    with c4:
        if 'ta' in data.columns:
            st.metric("ìµœì‹  ê¸°ì˜¨(Â°C)", f"{data['temperature'].iloc[0]:.1f}")
        else:
            st.metric("ìµœì‹  ê¸°ì˜¨(Â°C)", "N/A")

st.markdown("---")

# ===== ì±„íŒ… ìƒíƒœ =====
if "messages" not in st.session_state:
    st.session_state["messages"] = [{
        "role": "assistant",
        "content": "ì•ˆë…•í•˜ì„¸ìš”! âš¡ ì „ë ¥ìˆ˜ìš” ë°ì´í„° ë¶„ì„ ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤.\nì›í•˜ì‹œëŠ” ë¶„ì„ì„ ë§ì”€í•´ ì£¼ì„¸ìš”. (ì˜ˆ: ì–´ì œ í”¼í¬ ìˆ˜ìš” ì‹œê°„ì€?)"
    }]

# ê¸°ì¡´ ë©”ì‹œì§€ ì¶œë ¥
for m in st.session_state["messages"]:
    with st.chat_message(m["role"]):
        st.markdown(safe_str(m["content"]))

# ===== ì…ë ¥ ì²˜ë¦¬ =====
if prompt := st.chat_input("ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"):
    st.session_state["messages"].append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(safe_str(prompt))

    with st.chat_message("assistant"):
        with st.spinner("AI ì—ì´ì „íŠ¸ê°€ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤..."):
            try:
                # LangGraph ì›Œí¬í”Œë¡œìš° ì‚¬ìš©
                sel_backend = effective_backend
                if sel_backend == "ollama":
                    workflow = get_workflow_with_backend(
                        sel_backend, ollama_url, sql_model, ml_model, 
                        ollama_sql_model, ollama_ml_model
                    ) or default_workflow
                else:
                    workflow = get_workflow_with_backend(
                        sel_backend, None, sql_model, ml_model
                    ) or default_workflow
                
                # LangGraph ì›Œí¬í”Œë¡œìš° ì‹¤í–‰
                result = workflow.process_request(prompt)

                # ìµœì¢… ì‘ë‹µ ê°€ì ¸ì˜¤ê¸°
                response = result.get("final_response", "ì£„ì†¡í•©ë‹ˆë‹¤. ë¶„ì„ ê²°ê³¼ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                response = safe_str(response)
                
                # ë™ì  ì›Œí¬í”Œë¡œìš° ì°¨íŠ¸ í‘œì‹œ
                agent_sequence = result.get("agent_sequence", [])
                route_decision = result.get("route_decision", "")
                collaboration = result.get("collaboration", False)

                if agent_sequence:
                    with st.expander("ğŸ”„ **ì‹¤í–‰ëœ ì›Œí¬í”Œë¡œìš° ê²½ë¡œ**", expanded=False):
                        render_langgraph_workflow(
                            agent_sequence=agent_sequence,
                            route_decision=route_decision,
                            collaboration=False,
                            show_detailed=False
                        )

                    if "sql_analyst" in agent_sequence:
                        st.markdown("ğŸ” **SQL ë¶„ì„ê°€**ê°€ ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤")
                    if "ml_engineer" in agent_sequence:
                        st.markdown("ğŸ§  **ML ì—”ì§€ë‹ˆì–´**ê°€ ì°¸ì—¬í–ˆìŠµë‹ˆë‹¤")
                
                # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ
                execution_time = result.get("execution_time")
                if execution_time:
                    st.info(f"âš¡ ì‹¤í–‰ ì‹œê°„: {execution_time:.2f}ì´ˆ")
                
                # í˜‘ë ¥ ì •ë³´ í‘œì‹œ ì œê±° (ê°„ì†Œí™”)
                
                # ê²½ê³ /ì˜¤ë¥˜ í‘œì‹œ
                if result.get("warnings"):
                    for warning in result["warnings"]:
                        st.warning(f"âš ï¸ {warning}")
                
                if result.get("errors"):
                    for error in result["errors"]:
                        st.error(f"âŒ {error}")
                
                st.markdown(response)

                # SQL ê²°ê³¼ í‘œ/ì°¨íŠ¸ ì¶œë ¥
                sql_results = result.get("sql_results")
                if sql_results:
                    sql_raw = sql_results.get("raw_data") or []
                    for item in sql_raw:
                        text = item.get("result", "")
                        if "Query executed successfully" in text and ":\n" in text:
                            try:
                                json_part = text.split(":\n", 1)[1]
                                if "Total rows:" in json_part:
                                    json_part = json_part.split("\n\nTotal rows:")[0]
                                import json as _json
                                df = pd.DataFrame(_json.loads(json_part))
                                if not df.empty and len(df) <= 200:
                                    st.markdown("#### ğŸ“Š ì¡°íšŒ ê²°ê³¼")
                                    st.dataframe(df)
                                    # ê°„ë‹¨í•œ ë¼ì¸ ì°¨íŠ¸ (ì‹œê°„ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
                                    time_col = "time" if "time" in df.columns else ("ì¼ì‹œ" if "ì¼ì‹œ" in df.columns else ("t" if "t" in df.columns else ("date" if "date" in df.columns else None)))
                                    ycol = None
                                    for cand in ["power demand(MW)", "ta", "hm"]:
                                        if cand in df.columns:
                                            ycol = cand; break
                                    if time_col and ycol and len(df) > 2:
                                        fig = px.line(df, x=time_col, y=ycol, title=f"{ycol} ì¶”ì´", markers=True)
                                        fig.update_layout(xaxis_title="ì‹œê°„/ë‚ ì§œ", yaxis_title=ycol)
                                        st.plotly_chart(fig, use_container_width=True)
                            except Exception:
                                pass

                # ML ê²°ê³¼ ì½”ë“œ í‘œì‹œ
                ml_results = result.get("ml_results")
                if ml_results:
                    if ml_results.get("generated_code_path"):
                        st.markdown("#### ğŸ’» ìƒì„±ëœ ì½”ë“œ")
                        st.info(f"ì½”ë“œ íŒŒì¼: `{ml_results['generated_code_path']}`")
                        
                        # ìƒì„±ëœ ì½”ë“œ ë‚´ìš© í‘œì‹œ (ìˆìœ¼ë©´)
                        try:
                            with open(ml_results["generated_code_path"], "r", encoding="utf-8") as f:
                                code_content = f.read()
                            st.code(code_content, language="python")
                        except Exception:
                            st.warning("ì½”ë“œ íŒŒì¼ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    
                    if ml_results.get("strategy"):
                        st.markdown("#### ğŸ¯ ëª¨ë¸ë§ ì „ëµ")
                        st.markdown(safe_str(ml_results["strategy"]))

                st.session_state["messages"].append({"role": "assistant", "content": response})

            except Exception as e:
                err = f"ì£„ì†¡í•©ë‹ˆë‹¤. ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}"
                st.error(safe_str(err))
                st.session_state["messages"].append({"role": "assistant", "content": safe_str(err)})

st.markdown("---")
st.markdown("<div style='text-align:center;color:gray;'>âš¡ ì „ë ¥ìˆ˜ìš” AI ë¶„ì„ ì‹œìŠ¤í…œ | LangGraph Multi-Agent | Powered by zongseung</div>", unsafe_allow_html=True)
