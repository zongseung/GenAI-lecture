import os
import re
import textwrap
import unicodedata
from typing import Any, Dict, Optional, List, Tuple
from datetime import datetime
import sqlite3

from langchain_openai import ChatOpenAI
try:
    from langchain_community.chat_models import ChatOllama
except Exception:
    ChatOllama = None

from langchain.schema import HumanMessage, SystemMessage


# --------- Helpers ---------
def _strip_code_fences(s: str) -> str:
    """```python ... ``` í˜•íƒœë¡œ ê°ì‹¸ì ¸ ì˜¤ë©´ ë‚´ë¶€ë§Œ ì¶”ì¶œ."""
    if not s:
        return s
    pattern = r"^```(?:\w+)?\s*(.*?)\s*```$"
    m = re.match(pattern, s, flags=re.DOTALL)
    return m.group(1) if m else s

def _extract_section(text: str, tag: str) -> str:
    """
    LLM ì¶œë ¥ì—ì„œ íŠ¹ì • ì„¹ì…˜ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ.
    tag="CODE"ë©´ <<<CODE>>> ... <<<END-CODE>>> ì‚¬ì´ë¥¼ ë°˜í™˜.
    """
    if not text:
        return ""
    pattern = rf"<<<{tag}>>>\s*(.*?)\s*<<<END-{tag}>>>"
    m = re.search(pattern, text, flags=re.DOTALL | re.IGNORECASE)
    return (m.group(1) if m else "").strip()

def _versioned_filename(prefix: str, ext: str) -> str:
    now = datetime.now().strftime("%Y%m%d_%H%M%S")
    return f"{prefix}_{now}.{ext}"

def _safe_unicode(text: str) -> str:
    """ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™” ë° ASCII ì•ˆì „ ì²˜ë¦¬"""
    if not text:
        return text
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
    text = unicodedata.normalize('NFKC', text)
    # íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì¹˜í™˜
    replacements = {
        '\u201c': '"', '\u201d': '"',
        '\u2018': "'", '\u2019': "'",
        '\u2013': '-', '\u2014': '-',
        '\u2026': '...', '\u00a0': ' '
    }
    for unicode_char, ascii_char in replacements.items():
        text = text.replace(unicode_char, ascii_char)
    return text


# --------- Agent ----------
class MLEngineerAgent:
    """
    ì‚¬ìš©ìê°€ ìš”ì²­í•˜ë©´:
      1) LLMì´ PyTorch ê¸°ë°˜ì˜ 'ì¶©ë¶„íˆ ê¸´' ì‹¤í–‰ ê°€ëŠ¥ ì½”ë“œ ìƒì„± (ì£¼ì„ìœ¼ë¡œ ì„¤ëª…)
      2) ì½”ë“œì™€ requirements.txtë¥¼ ë¶„ë¦¬ ì¶œë ¥ â†’ íŒŒì¼ ì €ì¥
      3) ì„±ì°°(2íšŒ)ë¡œ ì½”ë“œ í’ˆì§ˆ/ì•ˆì •ì„±/ì¬í˜„ì„± ê°•í™”
    """
    def __init__(
        self,
        db_path: str,
        openai_api_key: str,
        backend: str = "openai",
        ollama_base_url: Optional[str] = None,
        ollama_model: Optional[str] = None,
        openai_model: str = "gpt-4o-mini",
    ):
        self.db_path = db_path
        backend = (backend or "openai").lower()

        if backend == "ollama" and ChatOllama is not None:
            self.llm = ChatOllama(
                model=ollama_model or "llama3",
                base_url=ollama_base_url or "http://localhost:11434",
                temperature=0.2,
            )
        else:
            self.llm = ChatOpenAI(
                model=openai_model,
                openai_api_key=openai_api_key,  # âœ… ì˜¬ë°”ë¥¸ ì¸ìëª…
                temperature=0.2,
            )

        # ì¼ê´€ëœ ê·œì¹™: "ì„¤ëª…ì€ ì£¼ì„ìœ¼ë¡œë§Œ", "ì½”ë“œ/ìš”êµ¬ì‚¬í•­ì€ ì •í•´ì§„ ì„¹ì…˜ìœ¼ë¡œë§Œ ì¶œë ¥"
        self.system_prompt = textwrap.dedent("""
        ë‹¹ì‹ ì€ ë”¥ëŸ¬ë‹/ë¨¸ì‹ ëŸ¬ë‹ **ì½”ë“œ ìƒì„± ì „ë¬¸ê°€**ì…ë‹ˆë‹¤.
        ë°˜ë“œì‹œ **ì‹¤í–‰ ê°€ëŠ¥í•œ PyTorch ê¸°ë°˜ Python ìŠ¤í¬ë¦½íŠ¸**ì™€ **requirements.txt**ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.

        ì¶œë ¥ ê·œì¹™(ì•„ì£¼ ì¤‘ìš”):
        - **ì„¤ëª…ì€ ì½”ë“œ ì£¼ì„(# ...)ìœ¼ë¡œë§Œ** ì‘ì„±í•˜ê³ , í…ìŠ¤íŠ¸ ì„¤ëª…ì€ ì ˆëŒ€ ì¶œë ¥í•˜ì§€ ë§ˆì„¸ìš”.
        - ì•„ë˜ì˜ ì„¹ì…˜ íƒœê·¸ë¥¼ ì •í™•íˆ ì§€ì¼œ ì¶œë ¥í•˜ì„¸ìš”.
          1) ì½”ë“œ ì„¹ì…˜:
             <<<CODE>>>
             # ì—¬ê¸°ì— ì‹¤í–‰ ê°€ëŠ¥í•œ Python ì½”ë“œ (í•˜ë‚˜ì˜ .py íŒŒì¼ë¡œ ì™„ê²°)
             <<<END-CODE>>>
          2) ìš”êµ¬ì‚¬í•­ ì„¹ì…˜:
             <<<REQS>>>
             # ì—¬ê¸°ì— requirements.txt ë‚´ìš© (íŒ¨í‚¤ì§€ ì¤„ë°”ê¿ˆ ë‚˜ì—´)
             <<<END-REQS>>>

        ë°ì´í„° ìŠ¤í‚¤ë§ˆ (SQLite 'my_table'):
        - time (TEXT, ISO8601 ë¬¸ìì—´)  â†’ pandasì—ì„œ datetimeìœ¼ë¡œ íŒŒì‹±
        - humanity (REAL, ìŠµë„), temperature (REAL, ê¸°ì˜¨)
        - spring/summer/autumn/winter (INTEGER, ê³„ì ˆ ë”ë¯¸)
        - weekday/weekend (INTEGER)
        - is_holiday_dummies (INTEGER), holiday_name (TEXT)
        - target: "power demand(MW)" (REAL, ì „ë ¥ìˆ˜ìš”)

        ì½”ë“œ í•„ìˆ˜ ìš”ê±´:
        - êµ¬ì¡°: load_data() â†’ build_dataset/dataloader â†’ build_model() â†’ train() â†’ evaluate() â†’ visualize() â†’ save_model()
        - ì‹œë“œ ê³ ì •(ì¬í˜„ì„±): random/np/torch ëª¨ë‘
        - ì¥ì¹˜: GPU ìˆìœ¼ë©´ cuda, ì—†ìœ¼ë©´ cpu ìë™ ì„ íƒ
        - ì „ì²˜ë¦¬: ê²°ì¸¡ì¹˜ ì²˜ë¦¬, ìŠ¤ì¼€ì¼ë§(StandardScaler ë“±)
        - ë°ì´í„° ë¶„ë¦¬: train/valid/test
        - í•™ìŠµ: ë°°ì¹˜/ì—í­, ì˜µí‹°ë§ˆì´ì €/ìŠ¤ì¼€ì¤„ëŸ¬(ì„ íƒ), ì¡°ê¸°ì¢…ë£Œ(ì„ íƒ)
        - í‰ê°€: MAE, MSE, RMSE, R2 ë“±
        - ì‹œê°í™”: í•™ìŠµ ê³¡ì„  ë° "ì˜ˆì¸¡ vs ì‹¤ì œ" ê·¸ë˜í”„ â†’ ./results/ í•˜ìœ„ì— ì €ì¥
        - ëª¨ë¸ ì €ì¥: ./models/ í•˜ìœ„ì— ì €ì¥ (os.makedirs ì‚¬ìš©)
        - ë¡œê¹…: logging ëª¨ë“ˆë¡œ ì£¼ìš” ë‹¨ê³„/ì§€í‘œ ì¶œë ¥
        - ìƒëŒ€ê²½ë¡œ ì‚¬ìš©, ê²½ë¡œ ìƒì„± ë³´ì¥(os.makedirs)
        - ì½”ë“œ ê¸¸ì´ëŠ” ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ (ì§§ì€ í† ì´ ì˜ˆì œ ê¸ˆì§€)

        requirements.txt ìƒì„± ì§€ì¹¨:
        - torch, torchmetrics(ì„ íƒ), numpy, pandas, scikit-learn, matplotlib, sqlite3(í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œì™¸), joblib ë“± í•„ìš” íŒ¨í‚¤ì§€ë§Œ
        - seabornì€ ì„ íƒ. ìµœì†Œ ì˜ì¡´ìœ¼ë¡œ êµ¬ì„±
        - ì •í™•í•œ íŒ¨í‚¤ì§€ëª…ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë‚˜ì—´ (ë²„ì „ ê³ ì •ì€ ì„ íƒ)
        """)

    # --- metadata: DB ì»¬ëŸ¼ ê°€ì ¸ì˜¤ê¸° (fallback í¬í•¨) ---
    def _get_table_columns(self) -> List[str]:
        fallback = [
            "time", "humanity", "temperature", "power demand(MW)",
            "holiday_name", "weekday", "weekend",
            "spring", "summer", "autumn", "winter",
            "is_holiday_dummies",
        ]
        try:
            if not os.path.exists(self.db_path):
                return fallback
            conn = sqlite3.connect(self.db_path)
            cur = conn.cursor()
            cur.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='my_table'")
            if not cur.fetchone():
                conn.close()
                return fallback
            cur.execute("PRAGMA table_info(my_table)")
            rows = cur.fetchall()
            conn.close()
            cols = [r[1] for r in rows] if rows else []
            return cols or fallback
        except Exception:
            return fallback

    # --- 1st draft generation ---
    def _gen_initial(self, request: str, columns: List[str]) -> str:
        prompt = textwrap.dedent(f"""
        ì‚¬ìš©ì ìš”ì²­: {request}

        ë°ì´í„°ë² ì´ìŠ¤: SQLite '{self.db_path}' â†’ 'my_table'
        ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {columns}

        ì•„ë˜ ì¶œë ¥ ê·œì¹™ì„ ë°˜ë“œì‹œ ë”°ë¥´ì„¸ìš”:
        - ì½”ë“œë§Œ ì£¼ì„ìœ¼ë¡œ ì„¤ëª…, í…ìŠ¤íŠ¸ ì„¤ëª… ê¸ˆì§€
        - ì½”ë“œëŠ” PyTorch ê¸°ë°˜ìœ¼ë¡œ, ì¶©ë¶„íˆ ìƒì„¸í•˜ê³  êµ¬ì¡°í™”ëœ ìŠ¤í¬ë¦½íŠ¸ë¡œ ì‘ì„±
        - ì½”ë“œ ì„¹ì…˜ê³¼ ìš”êµ¬ì‚¬í•­ ì„¹ì…˜ì„ ì •í™•íˆ êµ¬ë¶„í•´ íƒœê·¸ë¡œ ì¶œë ¥

        ì¶œë ¥ í˜•ì‹:
        <<<CODE>>>
        # ì‹¤í–‰ ê°€ëŠ¥í•œ Python ì½”ë“œ
        <<<END-CODE>>>
        <<<REQS>>>
        # requirements.txt ë‚´ìš©
        <<<END-REQS>>>
        """)
        resp = self.llm.invoke([SystemMessage(content=self.system_prompt),
                                HumanMessage(content=prompt)])
        return _safe_unicode(resp.content or "")

    # --- reflection round ---
    def _reflect(self, previous_code: str, focus: str) -> str:
        prompt = textwrap.dedent(f"""
        ë‹¤ìŒì€ ì´ì „ì— ìƒì„±í•œ ì½”ë“œ ì„¹ì…˜ì…ë‹ˆë‹¤:
        ```python
        {previous_code}
        ```

        ì§€ì‹œì‚¬í•­:
        - ì•„ë˜ ê°œì„  í¬ì¸íŠ¸ì— ì´ˆì ì„ ë§ì¶° **ì™„ì„±ëœ ì½”ë“œ**ì™€ **requirements.txt**ë¥¼ ë‹¤ì‹œ ì¶œë ¥í•˜ì„¸ìš”.
        - ì„¤ëª…ì€ ì½”ë“œ ì£¼ì„ìœ¼ë¡œë§Œ ì‘ì„±. í…ìŠ¤íŠ¸ ì„¤ëª… ê¸ˆì§€.
        - ë°˜ë“œì‹œ ì„¹ì…˜ íƒœê·¸ë¥¼ ì§€ì¼œ ì¶œë ¥:
          <<<CODE>>>
          # ê°œì„ ëœ ì „ì²´ Python ì½”ë“œ
          <<<END-CODE>>>
          <<<REQS>>>
          # requirements.txt
          <<<END-REQS>>>

        ê°œì„  í¬ì¸íŠ¸:
        {focus}
        """)
        resp = self.llm.invoke([SystemMessage(content=self.system_prompt),
                                HumanMessage(content=prompt)])
        return _safe_unicode(resp.content or "")

    def _save_files(self, code_text: str, reqs_text: str) -> Tuple[str, str, str]:
        os.makedirs("generated", exist_ok=True)
        filename = _versioned_filename("generated_model", "py")
        code_path = os.path.join("generated", filename)
        reqs_path = os.path.join("generated", "requirements.txt")
        status = "ì„±ê³µ"
        try:
            with open(code_path, "w", encoding="utf-8") as f:
                f.write(code_text.rstrip() + "\n")
            with open(reqs_path, "w", encoding="utf-8") as f:
                f.write(reqs_text.rstrip() + "\n")
        except Exception as e:
            status = f"ì‹¤íŒ¨: {e}"
        return code_path, reqs_path, status

    # --- Public API ---
    def create_model(self, request: str) -> Dict[str, Any]:
        columns = self._get_table_columns()

        # 1) ì´ˆê¸° ìƒì„±
        draft = self._gen_initial(request, columns)
        code_1 = _extract_section(_strip_code_fences(draft), "CODE")
        reqs_1 = _extract_section(_strip_code_fences(draft), "REQS")

        # 2) ì„±ì°° 1: í’ˆì§ˆ/ëª¨ë²”ì‚¬ë¡€/ë°ì´í„°ì²˜ë¦¬/ì„±ëŠ¥/ì‹œê°í™”
        focus_1 = textwrap.dedent("""
        1) ì½”ë“œ í’ˆì§ˆ/ëª¨ë“ˆì„±/ê°€ë…ì„± í–¥ìƒ
        2) PyTorch ëª¨ë²”ì‚¬ë¡€(ë””ë°”ì´ìŠ¤, ìµœì í™”, ë°°ì¹˜, ì²´í¬í¬ì¸íŠ¸)
        3) ë°ì´í„° ì „ì²˜ë¦¬/ìŠ¤ì¼€ì¼ë§/ì‹œê³„ì—´ ì²˜ë¦¬ ê°•í™”
        4) ì„±ëŠ¥(í•™ìŠµ ë£¨í”„/ë©”ëª¨ë¦¬ íš¨ìœ¨/ì¡°ê¸° ì¢…ë£Œ)
        5) ì‹œê°í™”(í•™ìŠµ ê³¡ì„ /ì˜ˆì¸¡ vs ì‹¤ì œ) ê°œì„  ë° ì €ì¥
        """)
        draft2 = self._reflect(code_1, focus_1)
        code_2 = _extract_section(_strip_code_fences(draft2), "CODE")
        reqs_2 = _extract_section(_strip_code_fences(draft2), "REQS")

        # 3) ì„±ì°° 2: ì•ˆì •ì„±/ë¡œê¹…/í•˜ì´í¼íŒŒë¼ë¯¸í„°/ì¬í˜„ì„±/ì‚¬ìš©ì„±
        focus_2 = textwrap.dedent("""
        1) ì‹¤í–‰ ì•ˆì •ì„±(ì˜ˆì™¸ ì²˜ë¦¬, ê²½ë¡œ ê²€ì¦)
        2) ë¡œê¹…(logging) ë° ì§„í–‰ ìƒí™© ì¶œë ¥
        3) í•˜ì´í¼íŒŒë¼ë¯¸í„° ê¸°ë³¸ê°’ê³¼ CLI ì¸ì ì²˜ë¦¬(optional)
        4) ì‹œë“œ ê³ ì • ë° ì¬í˜„ì„±
        5) ê²°ê³¼ë¬¼ ì €ì¥ êµ¬ì¡°(./models, ./results) ì—„ê²© ì¤€ìˆ˜
        """)
        draft3 = self._reflect(code_2, focus_2)
        code_final = _extract_section(_strip_code_fences(draft3), "CODE") or code_2 or code_1
        reqs_final = _extract_section(_strip_code_fences(draft3), "REQS") or reqs_2 or reqs_1

        # ë¹ˆ ì„¹ì…˜ ë°©ì–´
        if not code_final.strip():
            code_final = "# Fallback: ì½”ë“œ ìƒì„± ì‹¤íŒ¨\nprint('Code generation failed')\n"
        if not reqs_final.strip():
            # ê°€ì¥ ìµœì†Œ ì„¸íŠ¸
            reqs_final = "\n".join([
                "torch",
                "numpy",
                "pandas",
                "scikit-learn",
                "matplotlib",
                # sqlite3ëŠ” í‘œì¤€ ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ sqlite3 ëª¨ë“ˆì´ë¯€ë¡œ ë³„ë„ íŒ¨í‚¤ì§€ ë¶ˆí•„ìš”
            ])

        # ì €ì¥
        code_path, reqs_path, save_status = self._save_files(code_final, reqs_final)

        # ì‚¬ìš©ì ì•ˆë‚´
        analysis = textwrap.dedent(f"""
        ğŸ¯ **ë”¥ëŸ¬ë‹ ì½”ë“œ ìƒì„± ì™„ë£Œ (Reflection x2)**

        ğŸ“ ì½”ë“œ íŒŒì¼: `{code_path}`
        ğŸ“„ requirements: `{reqs_path}`
        ğŸ’¾ ì €ì¥ ìƒíƒœ: {save_status}

        ğŸ”„ ê°œì„  ê³¼ì •
        - ì´ˆì•ˆ â†’ ì„±ì°°â‘ (í’ˆì§ˆ/ëª¨ë²”ì‚¬ë¡€/ì„±ëŠ¥/ì‹œê°í™”) â†’ ì„±ì°°â‘¡(ì•ˆì •ì„±/ë¡œê¹…/ì¬í˜„ì„±/ì €ì¥êµ¬ì¡°)

        ğŸš€ ì‹¤í–‰ ë°©ë²•
        ```bash
        pip install -r {reqs_path}
        python {code_path}
        ```

        ê²°ê³¼ë¬¼ì€ `./results/`(ê·¸ë˜í”„/ë¦¬í¬íŠ¸), ëª¨ë¸ì€ `./models/`ì— ì €ì¥ë©ë‹ˆë‹¤.
        """)

        return {
            "agent_type": "ml_engineer",
            "request": request,
            "generated_code_path": code_path,
            "requirements_path": reqs_path,
            "analysis": analysis,
            "save_status": save_status,
            "raw_code_preview": code_final[:4000],  # í•„ìš”ì‹œ í™•ì¸ìš©
        }
