"""
ğŸš€ LangGraph ê¸°ë°˜ ë©€í‹°ì—ì´ì „íŠ¸ ì›Œí¬í”Œë¡œìš° (LLM Intelligent Routing + Fallback)

êµ¬ì¡°: Router â†’ Agent Executor â†’ Synthesizer (3ë…¸ë“œ ì„ í˜• í”Œë¡œìš°)
- Router: OpenAI LLM ê¸°ë°˜ intelligent routing (ì‹¤íŒ¨ ì‹œ í‚¤ì›Œë“œ fallback)
- Agent Executor: SQL / ML ë¼ìš°íŒ…ì— ë§ëŠ” ì—ì´ì „íŠ¸ ì‹¤í–‰
- Synthesizer: ìµœì¢… ê²°ê³¼ ì¢…í•© ë° ì‘ë‹µ ìƒì„±
"""
from typing import Dict, Any
import logging
import unicodedata
from langgraph.graph import StateGraph, END
from langchain_openai import ChatOpenAI
from langchain.schema import SystemMessage, HumanMessage

# ìŠ¤íŠ¸ë¦¬ë° ì§€ì› ì¶”ê°€
try:
    from langchain_teddynote.messages import stream_response
    STREAMING_AVAILABLE = True
except ImportError:
    STREAMING_AVAILABLE = False
    print("âš ï¸ langchain_teddynote ë¯¸ì„¤ì¹˜ - ìŠ¤íŠ¸ë¦¬ë° ë¹„í™œì„±í™”")

from .state import AgentState, create_initial_state
from ..agents.sql_analyst import SQLAnalystAgent
from ..agents.ml_engineer import MLEngineerAgent

logger = logging.getLogger(__name__)


def _sanitize_unicode(text: str) -> str:
    """ìœ ë‹ˆì½”ë“œ ë¬¸ì ì •ê·œí™” ë° ASCII ì•ˆì „ ì²˜ë¦¬"""
    if not isinstance(text, str):
        return str(text)
    
    # ìœ ë‹ˆì½”ë“œ ì •ê·œí™”
    text = unicodedata.normalize('NFKC', text)
    
    # íŠ¹ìˆ˜ ìœ ë‹ˆì½”ë“œ ë¬¸ì ì¹˜í™˜ (ASCII ì•ˆì „)
    replacements = {
        '\u201c': '"', '\u201d': '"',  # ìŠ¤ë§ˆíŠ¸ ë”°ì˜´í‘œ
        '\u2018': "'", '\u2019': "'",  # ìŠ¤ë§ˆíŠ¸ ë”°ì˜´í‘œ
        '\u2013': '-', '\u2014': '-',  # ëŒ€ì‹œ
        '\u2026': '...', '\u00a0': ' '  # ìƒëµë¶€í˜¸, ê³µë°±
    }
    
    for unicode_char, ascii_char in replacements.items():
        text = text.replace(unicode_char, ascii_char)
    
    return text


class EnergyLLMWorkflow:
    """ğŸš€ ì „ë ¥ìˆ˜ìš” AI ì›Œí¬í”Œë¡œìš° (3ë…¸ë“œ êµ¬ì¡°: Router â†’ Executor â†’ Synthesizer)"""
    
    def __init__(
        self,
        db_path: str,
        openai_api_key: str,
        backend: str = "openai",
        ollama_base_url: str = None,
        sql_model: str = "gpt-4o-mini",
        ml_model: str = "gpt-4o-mini",
        ollama_sql_model: str = None,
        ollama_ml_model: str = None,
    ):
        self.db_path = db_path
        self.openai_api_key = openai_api_key
        self.backend = backend
        self.ollama_base_url = ollama_base_url
        
        # ëª¨ë¸ ì„¤ì •
        self.models = {
            "sql_model": sql_model,
            "ml_model": ml_model,
            "ollama_sql_model": ollama_sql_model,
            "ollama_ml_model": ollama_ml_model,
        }
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        self._init_agents()
        
        # Supervisor LLM ì´ˆê¸°í™” (OpenAI + Ollama ì§€ì›)
        self.supervisor_llm = None
        self.ollama_llm = None
        
        # OpenAI LLM (ìš°ì„ )
        if openai_api_key:
            try:
                self.supervisor_llm = ChatOpenAI(
                    model="gpt-3.5-turbo",
                    openai_api_key=openai_api_key,
                    temperature=0.0
                )
                print("âœ… OpenAI Supervisor LLM ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ OpenAI LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        # Ollama LLM (ë°±ì—…)
        if ollama_base_url:
            try:
                from langchain_community.chat_models import ChatOllama
                self.ollama_llm = ChatOllama(
                    model=ollama_sql_model or "llama3.1:8b",
                    base_url=ollama_base_url,
                    temperature=0.0
                )
                print("âœ… Ollama Supervisor LLM ì´ˆê¸°í™” ì™„ë£Œ")
            except Exception as e:
                print(f"âŒ Ollama LLM ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        
        if not self.supervisor_llm and not self.ollama_llm:
            print("âš ï¸ ê²½ê³ : LLMì´ ì´ˆê¸°í™”ë˜ì§€ ì•ŠìŒ. í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°±ë§Œ ì‚¬ìš©ë©ë‹ˆë‹¤.")
        
        # LangGraph ì›Œí¬í”Œë¡œìš° êµ¬ì„±
        self.workflow = self._build_workflow()
        self.app = self.workflow.compile()
    
    def _init_agents(self):
        """SQL/ML ì—ì´ì „íŠ¸ ì´ˆê¸°í™”"""
        # SQL ë¶„ì„ê°€
        if self.backend == "ollama":
            self.sql_agent = SQLAnalystAgent(
                db_path=self.db_path,
                openai_api_key=self.openai_api_key,
                backend=self.backend,
                ollama_base_url=self.ollama_base_url,
                ollama_model=self.models["ollama_sql_model"] or self.models["sql_model"],
            )
        else:
            self.sql_agent = SQLAnalystAgent(
                db_path=self.db_path,
                openai_api_key=self.openai_api_key,
                backend=self.backend,
                openai_model=self.models["sql_model"],
            )
        
        # ML ì—”ì§€ë‹ˆì–´
        if self.backend == "ollama":
            self.ml_agent = MLEngineerAgent(
                db_path=self.db_path,
                openai_api_key=self.openai_api_key,
                backend=self.backend,
                ollama_base_url=self.ollama_base_url,
                ollama_model=self.models["ollama_ml_model"] or self.models["ml_model"],
            )
        else:
            self.ml_agent = MLEngineerAgent(
                db_path=self.db_path,
                openai_api_key=self.openai_api_key,
                backend=self.backend,
                openai_model=self.models["ml_model"],
            )
    
    def _build_workflow(self) -> StateGraph:
        """ê°„ë‹¨í•œ LangGraph ì›Œí¬í”Œë¡œìš°: supervisor â†’ {coder, researcher}"""
        workflow = StateGraph(AgentState)
        
        # ë…¸ë“œ ì¶”ê°€
        workflow.add_node("supervisor", self._supervisor_node)
        workflow.add_node("coder", self._coder_node)
        workflow.add_node("researcher", self._researcher_node)
        
        # ì‹œì‘ì  ì„¤ì •
        workflow.set_entry_point("supervisor")
        
        # ì¡°ê±´ë¶€ ë¼ìš°íŒ…: supervisorì—ì„œ ê²°ì •
        workflow.add_conditional_edges(
            "supervisor",
            self._route_condition,
            {
                "coder": "coder",
                "researcher": "researcher", 
                "FINISH": END
            }
        )
        
        # ê° ì—ì´ì „íŠ¸ì—ì„œ ë‹¤ì‹œ supervisorë¡œ (ìˆœí™˜ ê°€ëŠ¥)
        workflow.add_edge("coder", "supervisor")
        workflow.add_edge("researcher", "supervisor")
        
        return workflow
    
    # ========== Conditional Routing Function ==========
    def _route_condition(self, state: AgentState) -> str:
        """ì¡°ê±´ë¶€ ë¼ìš°íŒ… ê²°ì • í•¨ìˆ˜"""
        route = state.get("route_decision", "FINISH")
        print(f"ğŸ”€ ë¼ìš°íŒ… ê²°ì •: {route}")
        return route
    
    # ========== Supervisor Node ==========
    def _supervisor_node(self, state: AgentState) -> AgentState:
        """ğŸ¯ Supervisor: LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ë¼ìš°íŒ… ë° ìµœì¢… ì¢…í•©"""
        print(f"ğŸ‘¨â€ğŸ’¼ Supervisor ì‹œì‘: {state['user_request'][:50]}...")
        state["agent_sequence"].append("supervisor")

        # í˜„ì¬ ìƒíƒœ ë¶„ì„
        has_sql_result = bool(state.get("sql_results"))
        has_ml_result = bool(state.get("ml_results"))
        iteration_count = state.get("iteration_count", 0)
        
        # ë‹¤ì¤‘ì‘ì—… ì§€ì›: ì‚¬ìš©ìê°€ ì›í•˜ëŠ” ëª¨ë“  ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆëŠ”ì§€ ì²´í¬
        user_wants_both = self._user_wants_both_tasks(state["user_request"])
        
        # ì‘ì—… ì™„ë£Œ ì¡°ê±´ ì²´í¬
        work_complete = False
        if user_wants_both:
            # ë‘˜ ë‹¤ ì›í•˜ëŠ” ê²½ìš°: ë‘˜ ë‹¤ ì™„ë£Œë˜ì–´ì•¼ í•¨
            work_complete = has_sql_result and has_ml_result
        else:
            # í•˜ë‚˜ë§Œ ì›í•˜ëŠ” ê²½ìš°: í•´ë‹¹ ì‘ì—…ë§Œ ì™„ë£Œë˜ë©´ ë¨  
            work_complete = has_sql_result or has_ml_result
        
        # ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ ì²´í¬ ë˜ëŠ” ì‘ì—… ì™„ë£Œì‹œ ì¢…ë£Œ
        if work_complete or iteration_count >= 5:
            final_response = self._generate_final_response(state)
            if final_response:
                state["final_response"] = final_response
                state["route_decision"] = "FINISH"
                print(f"ğŸ ì‘ì—… ì™„ë£Œ - ì¢…ë£Œ (SQL: {has_sql_result}, ML: {has_ml_result})")
                return state
        
        # ë‹¤ìŒ ì‘ì—… ê²°ì •
        route = self._intelligent_routing(
            user_request=state["user_request"],
            has_sql_result=has_sql_result,
            has_ml_result=has_ml_result,
            iteration_count=iteration_count
        )
        
        state["iteration_count"] = iteration_count + 1
        state["route_decision"] = route
        print(f"ğŸ§  Supervisor ê²°ì •: {route} (ë°˜ë³µ: {state['iteration_count']})")
        return state
    
    def _generate_final_response(self, state: AgentState) -> str:
        """ìµœì¢… ì‘ë‹µ ìƒì„±"""
        responses = []
        
        # SQL ë¶„ì„ ê²°ê³¼
        if state.get("sql_analysis"):
            responses.append(f"ğŸ“Š **ë°ì´í„° ë¶„ì„ ê²°ê³¼**\n{state['sql_analysis']}")
        
        # ML ì½”ë“œ ê²°ê³¼  
        if state.get("ml_analysis"):
            responses.append(f"ğŸ¤– **ì½”ë“œ ìƒì„± ê²°ê³¼**\n{state['ml_analysis']}")
        
        # ì—ëŸ¬ê°€ ìˆëŠ” ê²½ìš°
        if state.get("errors") and not responses:
            error_msg = "\n".join([_sanitize_unicode(err) for err in state["errors"]])
            return f"âŒ **ì˜¤ë¥˜ ë°œìƒ**\n\n{error_msg}\n\nğŸ’¡ ë‹¤ì‹œ ì‹œë„í•´ë³´ì„¸ìš”."
        
        # ì •ìƒ ì‘ë‹µ
        if responses:
            return "\n\n".join(responses)
        
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­ì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤."
    
    def _user_wants_both_tasks(self, user_request: str) -> bool:
        """ì‚¬ìš©ìê°€ ë°ì´í„° ë¶„ì„ê³¼ ì½”ë“œ ìƒì„±ì„ ëª¨ë‘ ì›í•˜ëŠ”ì§€ íŒë‹¨"""
        text = user_request.lower()
        
        # ë‹¤ì¤‘ì‘ì—… í‚¤ì›Œë“œ
        both_keywords = [
            "ë¶„ì„í•˜ê³ ", "ë¶„ì„ í›„", "ê·¸ë¦¬ê³ ", "ë˜í•œ", "ë‹¤ìŒì—", "ì´ì–´ì„œ", 
            "ëª¨ë¸ë„", "ì½”ë“œë„", "ë¶„ì„ê³¼ ì½”ë“œ", "ë°ì´í„°ì™€ ëª¨ë¸", "ë‘˜ ë‹¤", "ëª¨ë‘"
        ]
        
        # ì½”ë“œ/ëª¨ë¸ ê´€ë ¨ í‚¤ì›Œë“œ
        coder_keywords = ["ëª¨ë¸", "ì½”ë“œ", "ì˜ˆì¸¡", "lstm", "ë”¥ëŸ¬ë‹", "ë¨¸ì‹ ëŸ¬ë‹", "ìƒì„±", "êµ¬í˜„", "ì•Œê³ ë¦¬ì¦˜"]
        # ë°ì´í„° ë¶„ì„ ê´€ë ¨ í‚¤ì›Œë“œ  
        researcher_keywords = ["ë¶„ì„", "ì¡°íšŒ", "ë°ì´í„°", "í†µê³„", "íŠ¸ë Œë“œ", "íŒ¨í„´", "ìµœê·¼", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ"]
        
        has_both_signal = any(k in text for k in both_keywords)
        has_coder = any(k in text for k in coder_keywords)
        has_researcher = any(k in text for k in researcher_keywords)
        
        # ëª…ì‹œì ìœ¼ë¡œ ë‘˜ ë‹¤ ì–¸ê¸‰í–ˆê±°ë‚˜, ë‘˜ ë‹¤ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ì„œ ì—°ê²°ì–´ê°€ ìˆìœ¼ë©´
        return has_both_signal or (has_coder and has_researcher)
    
    def _stream_llm_response(self, llm, messages, use_streaming=True):
        """LLM ì‘ë‹µ ìŠ¤íŠ¸ë¦¬ë° (ChatOllama ìš°ì„  ì§€ì›)"""
        if not use_streaming or not STREAMING_AVAILABLE:
            return llm.invoke(messages)
        
        try:
            # ChatOllamaì¸ ê²½ìš° ìŠ¤íŠ¸ë¦¬ë° ì‚¬ìš©
            if hasattr(llm, 'base_url'):  # Ollama ëª¨ë¸ ê°ì§€
                return stream_response(llm, messages)
            else:
                # OpenAIëŠ” ê¸°ë³¸ invoke ì‚¬ìš©
                return llm.invoke(messages)
        except Exception as e:
            print(f"ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨, ê¸°ë³¸ ëª¨ë“œë¡œ ì „í™˜: {e}")
            return llm.invoke(messages)
    
    def _intelligent_routing(self, user_request: str, has_sql_result: bool, 
                           has_ml_result: bool, iteration_count: int) -> str:
        """LLM ê¸°ë°˜ ì§€ëŠ¥í˜• ë¼ìš°íŒ… ê²°ì •"""
        
        # ë¬´í•œ ë£¨í”„ ë°©ì§€
        if iteration_count >= 3:
            return "FINISH"
            
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = []
        if has_sql_result:
            context.append("- ì´ë¯¸ ë°ì´í„° ë¶„ì„ì´ ì™„ë£Œë¨")
        if has_ml_result:
            context.append("- ì´ë¯¸ ì½”ë“œ ìƒì„±ì´ ì™„ë£Œë¨")
        
        context_text = "\n".join(context) if context else "- ì•„ì§ ìˆ˜í–‰ëœ ì‘ì—… ì—†ìŒ"
        
        routing_prompt = f"""
        ë‹¹ì‹ ì€ ì „ë ¥ìˆ˜ìš” ë¶„ì„ ì‹œìŠ¤í…œì˜ ì§€ëŠ¥í˜• ìˆ˜í¼ë°”ì´ì €ì…ë‹ˆë‹¤.
        ì‚¬ìš©ìì˜ ìš”ì²­ì„ ë¶„ì„í•˜ê³  í˜„ì¬ ìƒí™©ì„ ê³ ë ¤í•˜ì—¬ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ê²°ì •í•˜ì„¸ìš”.

        **ì‚¬ìš©ì ìš”ì²­:** {user_request}
        
        **í˜„ì¬ ìƒí™©:**
        {context_text}
        
        **ì„ íƒ ê°€ëŠ¥í•œ ë‹¤ìŒ ë‹¨ê³„:**
        1. **researcher** - ì „ë ¥ìˆ˜ìš” ë°ì´í„° ì¡°íšŒ, ë¶„ì„, í†µê³„, íŠ¸ë Œë“œ íƒìƒ‰
        2. **coder** - ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì½”ë“œ ìƒì„±, ì˜ˆì¸¡ ì•Œê³ ë¦¬ì¦˜ êµ¬í˜„
        3. **FINISH** - ì‘ì—… ì™„ë£Œ (ì‚¬ìš©ì ìš”êµ¬ì‚¬í•­ì´ ì¶©ì¡±ëœ ê²½ìš°)
        
        **íŒë‹¨ ê¸°ì¤€:**
        - ì‚¬ìš©ìê°€ ë°ì´í„° ë¶„ì„ì„ ì›í•˜ë©´ â†’ researcher
        - ì‚¬ìš©ìê°€ ì½”ë“œ/ëª¨ë¸ ìƒì„±ì„ ì›í•˜ë©´ â†’ coder  
        - ìš”êµ¬ì‚¬í•­ì´ ì´ë¯¸ ì¶©ì¡±ë˜ì—ˆìœ¼ë©´ â†’ FINISH
        - ë³µí•© ìš”ì²­ì¸ ê²½ìš° ìš°ì„ ìˆœìœ„ê°€ ë†’ì€ ê²ƒë¶€í„°
        
        ë°˜ë“œì‹œ researcher, coder, FINISH ì¤‘ í•˜ë‚˜ë§Œ ì¶œë ¥í•˜ì„¸ìš”.
        """

        try:
            # OpenAI ëª¨ë¸ ìš°ì„  ì‹œë„
            if self.supervisor_llm:
                messages = [
                    SystemMessage(content=routing_prompt),
                    HumanMessage(content=_sanitize_unicode(user_request))
                ]
                response = self._stream_llm_response(self.supervisor_llm, messages, use_streaming=False)
                route = _sanitize_unicode(response.content or "").strip().upper()
                
                # ìœ íš¨ì„± ê²€ì¦
                valid_routes = {"RESEARCHER", "CODER", "FINISH"}
                if route in valid_routes:
                    return route.lower() if route != "FINISH" else "FINISH"
        
        except Exception as e:
            print(f"OpenAI ë¼ìš°íŒ… ì‹¤íŒ¨: {_sanitize_unicode(str(e))}")
        
        # Ollama ë°±ì—… ì‹œë„ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        try:
            if hasattr(self, 'ollama_llm') and self.ollama_llm:
                messages = [
                    SystemMessage(content=routing_prompt),
                    HumanMessage(content=_sanitize_unicode(user_request))
                ]
                response = self._stream_llm_response(self.ollama_llm, messages, use_streaming=True)
                route = _sanitize_unicode(response.content or "").strip().upper()
                
                valid_routes = {"RESEARCHER", "CODER", "FINISH"}
                if route in valid_routes:
                    return route.lower() if route != "FINISH" else "FINISH"
                    
        except Exception as e:
            print(f"Ollama ë¼ìš°íŒ… ì‹¤íŒ¨: {_sanitize_unicode(str(e))}")
        
        # í´ë°±: í‚¤ì›Œë“œ ê¸°ë°˜ ë¼ìš°íŒ…
        return self._fallback_routing(user_request, has_sql_result, has_ml_result)
    
    def _fallback_routing(self, user_request: str, has_sql_result: bool, has_ml_result: bool) -> str:
        """í´ë°± ë¼ìš°íŒ… ë¡œì§ (ë‹¤ì¤‘ì‘ì—… ì§€ì›)"""
        text = user_request.lower()
        
        # ì‚¬ìš©ìê°€ ë‹¤ì¤‘ì‘ì—…ì„ ì›í•˜ëŠ”ì§€ ì²´í¬
        user_wants_both = self._user_wants_both_tasks(user_request)
        
        # ì™„ë£Œ ì¡°ê±´ ì²´í¬
        if user_wants_both:
            # ë‘˜ ë‹¤ ì›í•˜ëŠ” ê²½ìš°: ë‘˜ ë‹¤ ì™„ë£Œë˜ì–´ì•¼ ì¢…ë£Œ
            if has_sql_result and has_ml_result:
                return "FINISH"
        else:
            # í•˜ë‚˜ë§Œ ì›í•˜ëŠ” ê²½ìš°: í•´ë‹¹ ì‘ì—… ì™„ë£Œì‹œ ì¢…ë£Œ
            if has_sql_result or has_ml_result:
                return "FINISH"
        
        # ì½”ë“œ/ëª¨ë¸ ê´€ë ¨ í‚¤ì›Œë“œ
        coder_keywords = ["ëª¨ë¸", "ì½”ë“œ", "ì˜ˆì¸¡", "lstm", "ë”¥ëŸ¬ë‹", "ë¨¸ì‹ ëŸ¬ë‹", "ìƒì„±", "êµ¬í˜„", "ì•Œê³ ë¦¬ì¦˜"]
        # ë°ì´í„° ë¶„ì„ ê´€ë ¨ í‚¤ì›Œë“œ  
        researcher_keywords = ["ë¶„ì„", "ì¡°íšŒ", "ë°ì´í„°", "í†µê³„", "íŠ¸ë Œë“œ", "íŒ¨í„´", "ìµœê·¼", "í‰ê· ", "ìµœëŒ€", "ìµœì†Œ"]
        
        has_coder = any(k in text for k in coder_keywords)
        has_researcher = any(k in text for k in researcher_keywords)
        
        # ë‹¤ì¤‘ì‘ì—…ì¸ ê²½ìš° ìˆœì„œëŒ€ë¡œ ì²˜ë¦¬
        if user_wants_both or (has_coder and has_researcher):
            if not has_sql_result:
                return "researcher"  # ë°ì´í„° ë¶„ì„ ë¨¼ì €
            elif not has_ml_result:
                return "coder"       # ê·¸ ë‹¤ìŒ ì½”ë“œ ìƒì„±
            else:
                return "FINISH"
        
        # ë‹¨ì¼ ì‘ì—…ì¸ ê²½ìš°
        elif has_coder and not has_ml_result:
            return "coder"
        elif has_researcher and not has_sql_result:
            return "researcher"
        else:
            # ê¸°ë³¸ê°’: researcher ìš°ì„ 
            return "researcher" if not has_sql_result else "FINISH"
    
    # ========== Researcher Node ==========
    def _researcher_node(self, state: AgentState) -> AgentState:
        """ğŸ“Š Researcher: ë°ì´í„° ë¶„ì„ ë° ì—°êµ¬"""
        print("ğŸ“Š Researcher ì‹¤í–‰ ì‹œì‘")
        state["agent_sequence"].append("researcher")
        
        try:
            result = self.sql_agent.analyze(state["user_request"])
            state["sql_query"] = result.get("sql_query")
            state["sql_results"] = result
            state["sql_analysis"] = _sanitize_unicode(result.get("analysis", ""))
            print("âœ… Researcher ì‘ì—… ì™„ë£Œ - Supervisorë¡œ ë³µê·€")
            
        except Exception as e:
            safe_error = _sanitize_unicode(str(e))
            print(f"âŒ Researcher ì˜¤ë¥˜: {safe_error}")
            state["errors"].append(f"ë°ì´í„° ë¶„ì„ ì‹¤íŒ¨: {safe_error}")
        
        return state
    
    # ========== Coder Node ==========
    def _coder_node(self, state: AgentState) -> AgentState:
        """ğŸ¤– Coder: ì½”ë“œ ìƒì„± ë° ëª¨ë¸ë§"""
        print("ğŸ¤– Coder ì‹¤í–‰ ì‹œì‘")
        state["agent_sequence"].append("coder")
        
        try:
            result = self.ml_agent.create_model(state["user_request"])
            state["ml_code_path"] = result.get("generated_code_path")
            state["ml_results"] = result
            state["ml_analysis"] = _sanitize_unicode(result.get("analysis", ""))
            print("âœ… Coder ì‘ì—… ì™„ë£Œ - Supervisorë¡œ ë³µê·€")
            
        except Exception as e:
            safe_error = _sanitize_unicode(str(e))
            print(f"âŒ Coder ì˜¤ë¥˜: {safe_error}")
            state["errors"].append(f"ì½”ë“œ ìƒì„± ì‹¤íŒ¨: {safe_error}")
        
        return state
    
    # ========== Public API ==========
    def process_request(self, user_request: str) -> Dict[str, Any]:
        """ì‚¬ìš©ì ìš”ì²­ ì²˜ë¦¬"""
        import time
        start_time = time.time()
        
        initial_state = create_initial_state(
            user_request=user_request,
            backend=self.backend,
            models=self.models
        )
        
        try:
            result = self.app.invoke(initial_state)
            execution_time = time.time() - start_time
            
            return {
                "final_response": result.get("final_response"),
                "agent_sequence": result.get("agent_sequence", []),
                "sql_results": result.get("sql_results"),
                "ml_results": result.get("ml_results"),
                "route_decision": result.get("route_decision"),
                "collaboration": result.get("needs_collaboration", False),
                "errors": result.get("errors", []),
                "warnings": result.get("warnings", []),
                "execution_time": execution_time
            }
            
        except Exception as e:
            logger.error(f"ì›Œí¬í”Œë¡œìš° ì‹¤í–‰ ì˜¤ë¥˜: {e}")
            return {
                "final_response": f"âŒ ì‹œìŠ¤í…œ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}",
                "agent_sequence": ["error"],
                "errors": [str(e)]
            }
